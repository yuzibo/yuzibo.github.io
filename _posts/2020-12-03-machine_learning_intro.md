---
title: 机器学习入门
category: ml
layout: post
---
* content
{:toc}

目前，因为项目需要，需要简单地对机器学习相关的概念有一个初步了解，故总结记录在这里，其中，主要参考了这一系列文章:
https://scruel.gitee.io/ml-andrewng-notes/week2.html

#	机器学习入门

## 	什么是机器学习

两种定义(正式或非正式: 下棋，过滤垃圾邮件)

# 机器学习算法

`监督学习`和`无监督学习`，区别在于是否需要人工参与数据结果的标注。
监督学习(Supervised Learning):有反馈。给予计算机一定的输入和对应的结果(训练集)，建模拟合，让计算机推测结果。

## 监督学习

### 回归问题(Regression)

回归问题就是预测一系列的连续值；比如，根据照片推测年龄。

### 分类问题(Classification)

分类问题就是预测一系列的离散值。判别类的一些应用就是该类型。这里提了一下支持向量机。

## 无监督学习(Unsupervised Learning)

这里的训练集没有标记结果，即计算机执行完毕后不知道最后的结果的成功率。分为几个簇，也叫聚类算法。一般有两种:
1.	聚类(Clustering) 比如， 新闻聚合 天文数据分析 市场细分
2.	非聚类(Non-Clustering): 鸡尾酒问题: 语音识别一个人可以做的非常好，但是多人的话就识别不好。
1.3	单变量线性回归(Linear Regression with one variable)

### 模型

1.	房屋价格预测训练集，往年的价格，这不就是咱们当面学习的线性回归的方程嘛
2.	Model 训练集->训练算法, 然后输入x，预测y的值。接着上面的问答，关键就是在求系数。

### 代价函数(Cost function)

计算整个训练集所有损失函数之和的平均值。

### 损失函数(Loss function)

计算单个样本的误差。

#### 梯度下降(Gradient Descent)

解决在特征值很大的情况下，引入梯度下降的概念，让计算机自动找出最小化代价函数时对应的塞塔值。

#### 线性回归的梯度下降(Gradient Descent for Linear Regression)

这部分就和高等数学紧密相连了，感觉有些难度，暂时搁浅。
https://scruel.gitee.io/ml-andrewng-notes/week1.html

# 多变量线性回归(Linear Regression with Multiple Variables)

## 多特征(Multi feature)

# 逻辑回归

逻辑回归(Logistic Regression)

## 分类(Classification)

分类问题往往是判别的一种，预测的结果是离散值，逻辑回归算法就是被用于解决这类分类问题。

1. 垃圾邮件判断
2. 金融诈骗判断
3. 肿瘤诊断

以肿瘤诊断为例，是一个二元分类问题(Binary class problems),则 y $\in$ {0,1}, 0表示负向类(negative class),1为正向类(postive class)

## 假设函数表示(Hypothesis Represention)

为了使值分布在(0,1), 引入逻辑回归模型。

## 决策边界(Decision Boundary)

简单来说，决策边界就是分类的分界线。

## 多类别分类: 一对多

# 正则化(Regularization)

## 过拟合问题(The problem of Overfitting)

关于拟合的表现，可以分为三类问题:

> 欠拟合(Underfitting)
无法很好的拟合训练集中的数据，预测值和实际值的误差很大，这类情况被称为欠拟合。拟合模型比较简单（特征选少了）时易出现这类情况。类似于，你上课不好好听，啥都不会，下课也差不多啥都不会。

> 优良的拟合(Just right)
不论是训练集数据还是不在训练集中的预测数据，都能给出较为正确的结果。类似于，学霸学神！

> 过拟合(Overfitting)
能很好甚至完美拟合训练集中的数据，即趋向于0 ，但是对于不在训练集中的新数据，预测值和实际值的误差会很大，泛化能力弱，这类情况被称为过拟合。拟合模型过于复杂（特征选多了）时易出现这类情况。类似于，你上课跟着老师做题都会都听懂了，下课遇到新题就懵了不会拓展。

为了度量拟合表现，引入:
> 偏差(bias)

> 方差(Variance)

避免过拟合的方法有：

减少特征的数量

    1. 手动选取需保留的特征
    2. 使用模型选择算法来选取合适的特征(如 PCA 算法)
    3. 减少特征的方式易丢失有用的特征信息

正则化(Regularization)

    1. 可保留所有参数（许多有用的特征都能轻微影响结果）
    2. 减少/惩罚各参数大小(magnitude)，以减轻各参数对模型的影响程度
    3. 当有很多参数对于模型只有轻微影响时，正则化方法的表现很好

## 代价函数
很多时候由于特征数量过多，过拟合时我们很难选出要保留的特征，这时候应用正则化方法则是很好的选择。

## 线性回归正则化

# 神经网络

## 非线性解释

那么类比一下，把上图中的细胞核(nucleus)类比成工人，轴突(axon)类比传送带，树突(dendrite)则比类比成工人的双眼。一个又一个细胞体，从树突接收需要处理的信息，对其进行处理后，再经由轴突通过电信号把处理完的信息传递出去，直到理解信息的内容。当然啦，我们大脑的实际上还要更为复杂，而且一个人的神经元数目就比地球上所有流水线的工人之和还要多呢~

人工神经网络中，树突对应输入(input)，细胞核对应激活单元(activation unit)，轴突对应输出(output)。

我们一般把神经网络划分为三部分（注意，不是只有三层！），即输入层(input layer)，隐藏层(hidden layer)和输出层(output layer)。

图中的一个圈表示神经网络中的一个激活单元，输入层对应输入单元，隐藏层对应中间单元，输出层则对应输出单元。中间激活单元应用激活函数(activation_function)处理数据。